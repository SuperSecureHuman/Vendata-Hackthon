{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nbformat import reads, NO_CONVERT\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from typing import Dict\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "MIRROR_DIRECTORY = \"data\"\n",
    "DATASET_ID = \"vendata-code\"\n",
    "SERIALIZE_IN_CHUNKS = 10000\n",
    "FEATHER_FORMAT = \"ftr\"\n",
    "\n",
    "# Block the following formats.\n",
    "IMAGE = [\"png\", \"jpg\", \"jpeg\", \"gif\"]\n",
    "VIDEO = [\"mp4\", \"jfif\"]\n",
    "DOC = [\n",
    "    \"key\",\n",
    "    \"PDF\",\n",
    "    \"pdf\",\n",
    "    \"docx\",\n",
    "    \"xlsx\",\n",
    "    \"pptx\",\n",
    "]\n",
    "AUDIO = [\"flac\", \"ogg\", \"mid\", \"webm\", \"wav\", \"mp3\"]\n",
    "ARCHIVE = [\"jar\", \"aar\", \"gz\", \"zip\", \"bz2\"]\n",
    "MODEL = [\"onnx\", \"pickle\", \"model\", \"neuron\"]\n",
    "OTHERS = [\n",
    "    \"npy\",\n",
    "    \"index\",\n",
    "    \"inv\",\n",
    "    \"index\",\n",
    "    \"DS_Store\",\n",
    "    \"rdb\",\n",
    "    \"pack\",\n",
    "    \"idx\",\n",
    "    \"glb\",\n",
    "    \"gltf\",\n",
    "    \"len\",\n",
    "    \"otf\",\n",
    "    \"unitypackage\",\n",
    "    \"ttf\",\n",
    "    \"xz\",\n",
    "    \"pcm\",\n",
    "    \"opus\",\n",
    "]\n",
    "ANTI_FOMATS = tuple(IMAGE + VIDEO + DOC + AUDIO + ARCHIVE + OTHERS)\n",
    "\n",
    "\n",
    "def upload_to_hub(file_format: str, repo_id: str):\n",
    "    \"\"\"Moves all the files matching `file_format` to a folder and\n",
    "    uploads the folder to the Hugging Face Hub.\"\"\"\n",
    "    api = HfApi()\n",
    "    repo_id = create_repo(repo_id=repo_id, exist_ok=True, repo_type=\"dataset\").repo_id\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        os.makedirs(tmpdirname)\n",
    "        command = f\"mv *.{file_format} {tmpdirname}\"\n",
    "        _ = subprocess.run(command.split())\n",
    "        api.upload_folder(repo_id=repo_id, folder_path=tmpdirname, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "def filter_code_cell(cell) -> bool:\n",
    "    \"\"\"Filters a code cell w.r.t shell commands, etc.\"\"\"\n",
    "    only_shell = cell[\"source\"].startswith(\"!\")\n",
    "    only_magic = \"%%capture\" in cell[\"source\"]\n",
    "    if only_shell or only_magic:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def process_file(directory_name: str, file_path: str) -> Dict[str, str]:\n",
    "    \"\"\"Processes a single file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "            if file_path.endswith(\"ipynb\"):\n",
    "                # Code courtesy: Chansung Park and Sayak Paul.\n",
    "                code_cell_str = \"\"\n",
    "                notebook = reads(content, NO_CONVERT)\n",
    "\n",
    "                code_cells = [\n",
    "                    c\n",
    "                    for c in notebook[\"cells\"]\n",
    "                    if c[\"cell_type\"] == \"code\"\n",
    "                    if filter_code_cell(c)\n",
    "                ]\n",
    "\n",
    "                for cell in code_cells:\n",
    "                    code_cell_str += cell[\"source\"]\n",
    "                content = code_cell_str\n",
    "    except Exception:\n",
    "        content = \"\"\n",
    "\n",
    "    return {\n",
    "        \"repo_id\": directory_name,\n",
    "        \"file_path\": file_path,\n",
    "        \"content\": content,\n",
    "    }\n",
    "\n",
    "\n",
    "def read_repository_files(directory) -> pd.DataFrame:\n",
    "    \"\"\"Reads the files from the locally cloned repositories.\"\"\"\n",
    "    file_paths = []\n",
    "    df = pd.DataFrame(columns=[\"repo_id\", \"file_path\", \"content\"])\n",
    "    chunk_flag = 0\n",
    "\n",
    "    # Recursively find all files within the directory\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if not file_path.endswith(ANTI_FOMATS) and all(\n",
    "                k not in file_path for k in [\".git\", \"__pycache__\", \"xcodeproj\"]\n",
    "            ):\n",
    "                file_paths.append((os.path.dirname(root), file_path))\n",
    "\n",
    "    # Process files sequentially.\n",
    "    print(f\"Total file paths: {len(file_paths)}.\")\n",
    "    print(\"Reading file contents...\")\n",
    "\n",
    "    for i, (directory_name, file_path) in enumerate(tqdm(file_paths)):\n",
    "        file_content = process_file(directory_name, file_path)\n",
    "\n",
    "        if file_content[\"content\"] != \"\":\n",
    "            temp_df = pd.DataFrame.from_dict([file_content])\n",
    "            df = pd.concat([df, temp_df])\n",
    "\n",
    "            if (\n",
    "                SERIALIZE_IN_CHUNKS\n",
    "                and len(df) != 0\n",
    "                and (len(df) % SERIALIZE_IN_CHUNKS == 0)\n",
    "            ):\n",
    "                df_path = f\"df_chunk_{chunk_flag}_{len(df)}.{FEATHER_FORMAT}\"\n",
    "                print(f\"Serializing dataframe to {df_path}...\")\n",
    "                df.reset_index().to_feather(df_path)\n",
    "                del df\n",
    "                df = pd.DataFrame(columns=[\"repo_id\", \"file_path\", \"content\"])\n",
    "                chunk_flag += 1\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total file paths: 19257.\n",
      "Reading file contents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 10109/19257 [00:14<00:16, 571.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing dataframe to df_chunk_0_10000.ftr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19257/19257 [00:27<00:00, 708.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created, creating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = read_repository_files(MIRROR_DIRECTORY)\n",
    "print(\"DataFrame created, creating dataset...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455b7a232ed1450888438a8eea9523e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ac714313f44d2ab692614774fdd575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/SuperSecureHuman/vendata-code/commit/75accb774939d0ab8aabafade0402cea03210641', commit_message='Upload dataset', commit_description='', oid='75accb774939d0ab8aabafade0402cea03210641', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset.push_to_hub(DATASET_ID, private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
